{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 36.7MB/s eta 0:00:01     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b     | 180.9MB 36.7MB/s eta 0:00:02     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    | 190.7MB 36.7MB/s eta 0:00:01\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 36.5MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "spark = SparkSession.builder.getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-06-13 04:55:41--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-06-13 04:55:41--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-06-13 04:55:42--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: \u2018hmp.parquet\u2019\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-06-13 04:55:42 (27.5 MB/s) - \u2018hmp.parquet\u2019 saved [932997/932997]\n\n"
                }
            ],
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "#Linear Regression\n\ndata1 = {'partno':100, 'maxtemp':35, \"maxvibration\":12, 'asperity':0.32}\ndata1 = {'partno':101, 'maxtemp':35, \"maxvibration\":21, 'asperity':0.32}\ndata1 = {'partno':130, 'maxtemp':46, \"maxvibration\":3412, 'asperity':12.42}\ndata1 = {'partno':131, 'maxtemp':48,\"maxvibration\":3512, 'asperity':13.43}"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "w1=0\nw2=0\nw3=0\nw4=33\n\ndef linearPredict(data):\n    return w1+w2*data[\"maxtemp\"]+w3*data[\"maxvibration\"]+w4*data[\"asperity\"]"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "443.19"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "linearPredict(data1)"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "#Linear regression for HMP_Dataset\n\ndf.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------------------+-------------+\n|             label|        class|\n+------------------+-------------+\n| 8959.680239829991|Use_telephone|\n| 9737.511232342687|Standup_chair|\n| 12542.96539897962|     Eat_meat|\n|13225.945637269193|    Getup_bed|\n|15003.269043778426|  Drink_glass|\n+------------------+-------------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "df = df.createOrReplaceTempView(\"df\")\ndf_energy=spark.sql('''\nselect sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label,class from df group by class\n''')\n\ndf_energy.createOrReplaceTempView(\"df_energy\")\ndf_energy.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------------+-----------+\n|  x|  y|  z|              source|      class|            label|      class|\n+---+---+---+--------------------+-----------+-----------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n+---+---+---+--------------------+-----------+-----------------+-----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "#inner join to df for linear regression for x,y,z\ndf_join = spark.sql(\"\"\"\nselect * from df inner join df_energy on df.class = df_energy.class \n\"\"\")\n\ndf_join.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "#normalizing\nfrom pyspark.ml.feature import VectorAssembler,Normalizer\n\nva = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\",outputCol=\"features_norm\",p=1.0)\n\n# normalizer.transhow(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "#Linear regression in pyspark\nfrom pyspark.ml.regression import LinearRegression\n\nlinearRegression = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\n|  x|  y|  z|              source|      class|            label|      class|        features|       features_norm|        prediction|\n+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,49.0,35.0]|[0.20754716981132...|12586.729735016828|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,49.0,35.0]|[0.20754716981132...|12586.729735016828|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,35.0]|[0.20183486238532...|12542.703337345756|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,35.0]|[0.20183486238532...|12542.703337345756|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,52.0,34.0]|[0.19626168224299...|12573.865911821156|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,51.0,34.0]|[0.20560747663551...|12541.076564471234|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,50.0,35.0]|[0.19047619047619...|12666.983895607029|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,34.0]|[0.20370370370370...|12526.401098580878|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,50.0,34.0]|[0.20754716981132...|12555.752030361591|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,51.0,35.0]|[0.20370370370370...|12557.378803236114|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,51.0,33.0]|[0.2,0.4857142857...|12572.239138946636|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,50.0,34.0]|[0.19230769230769...| 12650.68165684215|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,49.0,33.0]|[0.20388349514563...|12601.590070727349|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,49.0,33.0]|[0.20388349514563...|12601.590070727349|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,51.0,35.0]|[0.18867924528301...|12652.308429716672|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,49.0,34.0]|[0.17821782178217...|12760.286749213064|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[19.0,48.0,34.0]|[0.18811881188118...|12727.497401863142|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[16.0,53.0,34.0]|[0.15533980582524...|12796.514512132195|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,52.0,35.0]|[0.17142857142857...|12732.562590306872|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,51.0,32.0]|[0.17821782178217...|12698.331339902592|\n+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[va,normalizer,linearRegression])\n\nmodel = pipeline.fit(df_join)\n\npredictions = model.transform(df_join)\npredictions.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.03259100556263628"
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#r2 measure of the model built\nmodel.stages[2].summary.r2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "#Logistic Regression\n"
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-06-13 05:14:37--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-06-13 05:14:37--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-06-13 05:14:37--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: \u2018hmp.parquet\u2019\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-06-13 05:14:37 (29.3 MB/s) - \u2018hmp.parquet\u2019 saved [932997/932997]\n\n"
                }
            ],
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "df.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": "# df = df.createOrReplaceTempView(\"df\")"
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]"
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  x|  y|  z|              source|        class|label|       features|       features_norm|       rawPrediction|         probability|prediction|\n+---+---+---+--------------------+-------------+-----+---------------+--------------------+--------------------+--------------------+----------+\n|  0| 11| 38|Accelerometer-201...|Sitdown_chair|  8.0|[0.0,11.0,38.0]|[0.0,0.2244897959...|[1.25748310211891...|[0.20691496224784...|       0.0|\n|  0| 16| 31|Accelerometer-201...|    Getup_bed|  1.0|[0.0,16.0,31.0]|[0.0,0.3404255319...|[1.25748310211891...|[0.20691496224784...|       0.0|\n|  0| 26| 15|Accelerometer-201...| Climb_stairs|  4.0|[0.0,26.0,15.0]|[0.0,0.6341463414...|[1.25748310211891...|[0.20691496224784...|       0.0|\n|  0| 27| 37|Accelerometer-201...|  Brush_teeth|  6.0|[0.0,27.0,37.0]|[0.0,0.421875,0.5...|[1.25748310211891...|[0.20691496224784...|       0.0|\n|  0| 28| 28|Accelerometer-201...|  Brush_teeth|  6.0|[0.0,28.0,28.0]|       [0.0,0.5,0.5]|[1.25748310211891...|[0.20691496224784...|       0.0|\n+---+---+---+--------------------+-------------+-----+---------------+--------------------+--------------------+--------------------+----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "#logistic\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nprediction.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- features_norm: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n"
                }
            ],
            "source": "prediction.printSchema()"
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.20535123804826652"
                    },
                    "execution_count": 53,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#performance evaluation\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nMulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction) "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Using random forest classifier for classification\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier\n"
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "df.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----+---------------+---------------+--------------------+--------------------+----------+\n|  x|  y|  z|              source|      class|label|       features|indexedFeatures|       rawPrediction|         probability|prediction|\n+---+---+---+--------------------+-----------+-----+---------------+---------------+--------------------+--------------------+----------+\n|  0| 28| 28|Accelerometer-201...|Brush_teeth|  6.0|[0.0,28.0,28.0]|[0.0,28.0,28.0]|[3.10353327062450...|[0.31035332706245...|       0.0|\n|  0| 28| 48|Accelerometer-201...|Brush_teeth|  6.0|[0.0,28.0,48.0]|[0.0,28.0,48.0]|[1.11450868665456...|[0.11145086866545...|       1.0|\n|  0| 29| 34|Accelerometer-201...|       Walk|  0.0|[0.0,29.0,34.0]|[0.0,29.0,34.0]|[3.10353327062450...|[0.31035332706245...|       0.0|\n|  0| 29| 39|Accelerometer-201...|Brush_teeth|  6.0|[0.0,29.0,39.0]|[0.0,29.0,39.0]|[3.10353327062450...|[0.31035332706245...|       0.0|\n|  0| 30| 46|Accelerometer-201...|Brush_teeth|  6.0|[0.0,30.0,46.0]|[0.0,30.0,46.0]|[1.20364075713179...|[0.12036407571317...|       1.0|\n+---+---+---+--------------------+-----------+-----+---------------+---------------+--------------------+--------------------+----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorIndexer\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\n# normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n\nfeatureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", numTrees=10)\n\npipeline = Pipeline(stages=[indexer, vectorAssembler, featureIndexer,rf])\nmodel = pipeline.fit(df_train)\nprediction = model.transform(df_test)\nprediction.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------------------+-----------------+----------------+\n|        prediction|            label|        features|\n+------------------+-----------------+----------------+\n|12586.729735016828|11785.39634462923|[22.0,49.0,35.0]|\n|12586.729735016828|11785.39634462923|[22.0,49.0,35.0]|\n|12542.703337345756|11785.39634462923|[22.0,52.0,35.0]|\n|12542.703337345756|11785.39634462923|[22.0,52.0,35.0]|\n|12573.865911821156|11785.39634462923|[21.0,52.0,34.0]|\n+------------------+-----------------+----------------+\nonly showing top 5 rows\n\nTest Error = 1\nVectorIndexer_39340e3a01f5\n"
                }
            ],
            "source": "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only"
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.44113668630453984"
                    },
                    "execution_count": 73,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nMulticlassClassificationEvaluator().setMetricName(\"accuracy\").evaluate(prediction) "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}